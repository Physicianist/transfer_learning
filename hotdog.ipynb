{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13379631,"sourceType":"datasetVersion","datasetId":8488902}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport urllib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom skimage import io, transform\nfrom torchvision import models\nfrom torch.utils.data import Dataset, SubsetRandomSampler\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\ndevice = torch.device(\"cuda:0\")\nprint(f\"Using device: {device}\")\n\nclass HotdogRecognitionDataset(Dataset):\n    def __init__(self, folder, transform=None):\n        self.folder = folder\n        self.transform = transform\n        self.files = os.listdir(folder)\n        self.hotdog_prefixes = ['chili-dog', 'frankfurter', 'hotdog']\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, index):\n        img_name = self.files[index]\n        img_path = os.path.join(self.folder, img_name)\n        image = Image.open(img_path)\n        y = 0\n        for prefix in self.hotdog_prefixes:\n            if img_name.startswith(prefix):\n                y = 1\n                break\n        if self.transform:\n            image = self.transform(image)\n        return image, y, img_name\n\ndef visualize_samples(dataset, indices, title=None):\n    n = len(indices)\n    fig, axes = plt.subplots(1, n, figsize=(20, 3))\n    if title:\n        plt.suptitle(title, fontsize=16)\n    \n    for i, index in enumerate(indices):\n        x, y, img_name = dataset[index]\n        ax = axes[i]\n        ax.imshow(x)\n        ax.set_title(f\"Label: {y}\", fontsize=12)\n        ax.grid(False)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n# Путь к обучающим данным\ntrain_path = '/kaggle/input/hotdog-dataset-zip/Задание 5. Нейронные сети/train/train_kaggle'\ntest_path = '/kaggle/input/hotdog-dataset-zip/Задание 5. Нейронные сети/test/test_kaggle'\n\ntrain_dataset = HotdogRecognitionDataset(train_path,\n                       transform=transforms.Compose([\n                           transforms.Resize((224, 224)),\n                           transforms.ToTensor(),\n                           # Use mean and std for pretrained models\n                           # https://pytorch.org/docs/stable/torchvision/models.html\n                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n                       ])\n                      )\ntest_dataset = HotdogRecognitionDataset(test_path,\n                       transform=transforms.Compose([\n                           transforms.Resize((224, 224)),\n                           transforms.ToTensor(),\n                           # Use mean and std for pretrained models\n                           # https://pytorch.org/docs/stable/torchvision/models.html\n                           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n                       ])\n                      )\nbatch_size = 64\n\ndata_size = len(train_dataset)\nvalidation_fraction = .2\n\n\nval_split = int(np.floor((validation_fraction) * data_size))\nindices = list(range(data_size))\nnp.random.seed(42)\nnp.random.shuffle(indices)\n\nval_indices, train_indices = indices[:val_split], indices[val_split:]\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                           sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n                                         sampler=val_sampler)\n\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n\ndef compute_accuracy(model, loader):\n    model.eval()  # Enter evaluation mode\n    correct_samples = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for x, y, _ in loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            predictions = model(x)\n            correct_samples += (predictions.argmax(dim=1) == y).sum().item()\n            total_samples += y.size(0)\n\n    return correct_samples / total_samples\n\ndef train_model_with_early_stopping(model, train_loader, val_loader, loss, optimizer, num_epochs, patience=5):\n    best_val_acc = 0\n    patience_counter = 0\n    best_model_state = None\n    \n    loss_history = []\n    train_history = []\n    val_history = []\n    \n    for epoch in range(num_epochs):\n        model.train()\n        loss_accum = 0\n        correct_samples = 0\n        total_samples = 0\n        \n        for i_step, (x, y, _) in enumerate(train_loader):\n            x_gpu = x.to(device)\n            y_gpu = y.to(device)\n            \n            prediction = model(x_gpu)\n            loss_value = loss(prediction, y_gpu)\n            \n            optimizer.zero_grad()\n            loss_value.backward()\n            optimizer.step()\n            \n            _, indices = torch.max(prediction, 1)\n            correct_samples += torch.sum(indices == y_gpu).item()\n            total_samples += y.shape[0]\n            loss_accum += loss_value.item()\n        \n        scheduler.step()\n        \n        ave_loss = loss_accum / len(train_loader)\n        train_accuracy = correct_samples / total_samples\n        val_accuracy = compute_accuracy(model, val_loader)\n        \n        loss_history.append(ave_loss)\n        train_history.append(train_accuracy)\n        val_history.append(val_accuracy)\n        \n        print(f\"Epoch {epoch+1}: Loss: {ave_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n        \n        # Ранняя остановка\n        if val_accuracy > best_val_acc:\n            best_val_acc = val_accuracy\n            best_model_state = model.state_dict().copy()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            \n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n            \n    model.load_state_dict(best_model_state)\n    return loss_history, train_history, val_history\n\n# Thanks to https://discuss.pytorch.org/t/imagenet-classes/4923/2\ndef load_imagenet_classes():\n    classes_json = urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json').read()\n    classes = json.loads(classes_json)\n\n    #Process it to return dict of class index to name\n    return { int(k): v[-1] for k, v in classes.items()}\n\ndef visualize_resnet_predictions(model, dataset, num_samples=10):\n    \"\"\"\n    Тестирует ResNet18 на случайных изображениях и визуализирует предсказания\n    \"\"\"\n    model.eval()\n    # Загружаем классы ImageNet\n    imagenet_classes = load_imagenet_classes()\n    indices = np.random.choice(len(dataset), num_samples, replace=False)\n    plt.figure(figsize=(20, 8))\n\n    for i, idx in enumerate(indices):\n        image_tensor, true_label, filename = dataset[idx]\n        \n        with torch.no_grad():\n            # Добавляем размерность батча и перемещаем на устройство\n            prediction = model(image_tensor.unsqueeze(0).to(device))\n            values, indices = torch.max(prediction, 1)\n        \n        pred_class_idx = indices.item()\n        pred_class_name = imagenet_classes[pred_class_idx]\n        confidence = torch.softmax(prediction, dim=1)[0][pred_class_idx].item()\n        \n        # Денормализуем изображение для отображения\n        mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n        std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n        image_denorm = image_tensor * std + mean\n        image_denorm = torch.clamp(image_denorm, 0, 1)\n        image_np = image_denorm.permute(1, 2, 0).numpy()\n\n        plt.subplot(2, 5, i + 1)\n        plt.imshow(image_np)\n        plt.title(f\"True: {'Hotdog' if true_label == 1 else 'Not Hotdog'}\\nPred: {pred_class_name}\\nConf: {confidence:.3f}\")\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n    \n#Тренируем только последний слой\n# for param in model.parameters():\n#     param.requires_grad = False\n# model.fc = nn.Linear(512, 2)\n# model.fc = model.fc.to(device)\n# loss = nn.CrossEntropyLoss()\n# optimizer = optim.SGD( model.parameters(), lr=0.001, momentum=0.9)\n# loss_history, train_history, val_history = train_model(model, train_loader, val_loader, loss, optimizer, 2)\n\nmodel = models.resnet18(pretrained=True)\nmodel = model.to(device)\n#Тренируем всю модель\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\nmodel.fc = model.fc.to(device)\n# Разные learning rates для разных частей\nparameters = [\n    {'params': model.fc.parameters(), 'lr': 0.001},  # Быстрее обучаем новый слой\n    {'params': [p for n, p in model.named_parameters() if 'fc' not in n], 'lr': 0.0001}]\nloss = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(parameters, lr=0.001, weight_decay=1e-4)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\nloss_history, train_history, val_history = train_model_with_early_stopping(\n    model, train_loader, val_loader, loss, optimizer, 20, patience=5\n)\n\nfrom torch.utils.data.sampler import Sampler\nfrom torch.utils.data import DataLoader\nimport sklearn.metrics as metrics\n\nclass SubsetSampler(Sampler):\n\n    def __init__(self, indices):\n        self.indices = indices\n\n    def __iter__(self):\n        return (self.indices[i] for i in range(len(self.indices)))\n\n    def __len__(self):\n        return len(self.indices)\n        \ndef evaluate_model(model, dataset, indices):\n    sampler = SubsetSampler(indices)\n    loader = DataLoader(dataset, batch_size=32, sampler=sampler)\n    all_predictions = []\n    all_ground_truth = []\n    with torch.no_grad():\n        for batch in loader:\n            x, y, _ = batch\n            x = x.to(device)\n            y = y.to(device)\n            outputs = model(x)\n            _, predicted = torch.max(outputs, 1)\n            all_predictions.extend(predicted.cpu().numpy())\n            all_ground_truth.extend(y.cpu().numpy())\n            \n    predictions = np.array(all_predictions, dtype=bool)\n    ground_truth = np.array(all_ground_truth, dtype=bool)\n    return predictions, ground_truth\n\n\n\n\ndef binary_classification_metrics(prediction, ground_truth):\n    prediction_int = prediction.astype(int)\n    ground_truth_int = ground_truth.astype(int)\n    precision = metrics.precision_score(ground_truth_int, prediction_int)\n    recall = metrics.recall_score(ground_truth_int, prediction_int)\n    f1 = metrics.f1_score(ground_truth_int, prediction_int)\n    return precision, recall, f1\n\npredictions, ground_truth = evaluate_model(model, train_dataset, val_indices)\nprecision, recall, f1 = binary_classification_metrics(predictions, ground_truth)\nprint(\"F1: %4.3f, P: %4.3f, R: %4.3f\" % (precision, recall, f1))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-14T18:58:39.483880Z","iopub.execute_input":"2025-10-14T18:58:39.484620Z","iopub.status.idle":"2025-10-14T19:04:32.858485Z","shell.execute_reply.started":"2025-10-14T18:58:39.484596Z","shell.execute_reply":"2025-10-14T19:04:32.857796Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda:0\nEpoch 1: Loss: 0.1950, Train Acc: 0.9169, Val Acc: 0.9598\nEpoch 2: Loss: 0.0331, Train Acc: 0.9929, Val Acc: 0.9609\nEpoch 3: Loss: 0.0123, Train Acc: 0.9967, Val Acc: 0.9533\nEpoch 4: Loss: 0.0057, Train Acc: 0.9992, Val Acc: 0.9641\nEpoch 5: Loss: 0.0024, Train Acc: 0.9997, Val Acc: 0.9576\nEpoch 6: Loss: 0.0011, Train Acc: 1.0000, Val Acc: 0.9630\nEpoch 7: Loss: 0.0006, Train Acc: 1.0000, Val Acc: 0.9641\nEpoch 8: Loss: 0.0008, Train Acc: 1.0000, Val Acc: 0.9641\nEpoch 9: Loss: 0.0003, Train Acc: 1.0000, Val Acc: 0.9630\nEarly stopping at epoch 9\nF1: 0.948, P: 0.928, R: 0.938\n","output_type":"stream"}],"execution_count":24}]}